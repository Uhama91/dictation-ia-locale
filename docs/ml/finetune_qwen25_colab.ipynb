{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2.5-0.5B pour post-traitement dict√©e FR\n",
    "\n",
    "**Objectif** : Entra√Æner Qwen2.5-0.5B-Instruct avec LoRA pour corriger les sorties brutes de Whisper en fran√ßais.\n",
    "\n",
    "**T√¢ches couvertes** :\n",
    "- Task 20 ‚Äî G√©n√©ration du dataset (paires brut Whisper ‚Üí texte corrig√©)\n",
    "- Task 21 ‚Äî Fine-tuning LoRA/QLoRA sur Qwen2.5-0.5B\n",
    "- Task 22 ‚Äî Export du mod√®le au format GGUF pour int√©gration dans l'app\n",
    "\n",
    "## Instructions\n",
    "1. **Runtime** ‚Üí Modifier le type d'ex√©cution ‚Üí GPU (T4 gratuit ou mieux)\n",
    "2. Cliquer **Ex√©cution** ‚Üí **Tout ex√©cuter**\n",
    "3. √Ä la fin (~2-4h sur T4), t√©l√©charger `qwen25-dictation-fr-q4.gguf`\n",
    "4. Placer le fichier dans `~/Library/Application Support/dictation-ia/models/`\n",
    "\n",
    "---\n",
    "**Pr√©requis** : Aucun. Tout est automatis√©.\n",
    "\n",
    "**Co√ªt** : 0‚Ç¨ (Colab gratuit T4) ou ~3-5‚Ç¨ (Colab Pro A100, 4√ó plus rapide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. V√©rification GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
    "                       capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    gpu_info = result.stdout.strip()\n",
    "    print(f'‚úÖ GPU d√©tect√© : {gpu_info}')\n",
    "else:\n",
    "    raise RuntimeError('‚ùå Aucun GPU d√©tect√©. Allez dans Runtime ‚Üí Modifier le type d\\'ex√©cution ‚Üí GPU')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%capture\n",
    "!pip install -q \\\n",
    "    transformers==4.46.0 \\\n",
    "    peft==0.13.0 \\\n",
    "    trl==0.12.0 \\\n",
    "    datasets==3.1.0 \\\n",
    "    bitsandbytes==0.44.1 \\\n",
    "    accelerate==1.1.0 \\\n",
    "    llama-cpp-python==0.3.2 \\\n",
    "    huggingface_hub\n",
    "\n",
    "print('‚úÖ D√©pendances install√©es')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MODEL_ID       = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "OUTPUT_DIR     = '/content/qwen25-dictation-fr'\n",
    "GGUF_NAME      = 'qwen25-dictation-fr-q4.gguf'\n",
    "MAX_SEQ_LEN    = 256   # Phrases dict√©es courtes ‚Äî √©conomise la VRAM\n",
    "EPOCHS         = 3\n",
    "BATCH_SIZE     = 8\n",
    "GRAD_ACCUM     = 4     # Batch effectif = 32\n",
    "LEARNING_RATE  = 2e-4\n",
    "LORA_R         = 16\n",
    "LORA_ALPHA     = 32\n",
    "LORA_DROPOUT   = 0.05\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "import os, json, random\n",
    "from pathlib import Path\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f'‚úÖ Config OK ‚Äî mod√®le cible : {MODEL_ID}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. G√©n√©ration du dataset (Task 20)\n",
    "\n",
    "G√©n√®re ~10 000 paires *(sortie brute Whisper ‚Üí texte corrig√©)* en fran√ßais.\n",
    "Les erreurs typiques de Whisper FR sont simul√©es : fillers, √©lisions mal form√©es, ponctuation manquante, b√©gaiements."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re, random\n",
    "\n",
    "# ‚îÄ‚îÄ Corpus de phrases fran√ßaises propres ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CLEAN_SENTENCES = [\n",
    "    # Quotidien\n",
    "    \"Je voudrais r√©server une table pour deux personnes ce soir.\",\n",
    "    \"Peux-tu m'envoyer le document par email s'il te pla√Æt ?\",\n",
    "    \"La r√©union est pr√©vue pour demain matin √† neuf heures.\",\n",
    "    \"Il faudrait qu'on discute de ce projet avant la fin de la semaine.\",\n",
    "    \"J'ai besoin d'aide pour configurer mon ordinateur.\",\n",
    "    \"Le rapport doit √™tre remis au plus tard vendredi.\",\n",
    "    \"Tu peux me rappeler dans une heure ?\",\n",
    "    \"On se retrouve √† la gare √† midi moins le quart.\",\n",
    "    \"N'oublie pas d'acheter du pain sur le chemin du retour.\",\n",
    "    \"La pr√©sentation s'est tr√®s bien pass√©e, tout le monde √©tait satisfait.\",\n",
    "    # Professionnel\n",
    "    \"Suite √† notre √©change t√©l√©phonique, je vous adresse ce compte-rendu.\",\n",
    "    \"Merci de bien vouloir confirmer votre pr√©sence avant jeudi.\",\n",
    "    \"Je reste disponible pour toute question compl√©mentaire.\",\n",
    "    \"Le budget pr√©visionnel a √©t√© valid√© par la direction.\",\n",
    "    \"Nous devons imp√©rativement respecter les d√©lais contractuels.\",\n",
    "    \"L'√©quipe technique travaille sur la r√©solution du probl√®me.\",\n",
    "    \"Ce point sera abord√© lors de la prochaine r√©union de suivi.\",\n",
    "    \"Je vous transmets les documents demand√©s en pi√®ce jointe.\",\n",
    "    \"Pouvez-vous me confirmer les sp√©cifications techniques requises ?\",\n",
    "    \"La mise en production est planifi√©e pour le premier du mois prochain.\",\n",
    "    # Technique\n",
    "    \"Il faut mettre √† jour la base de donn√©es avant le d√©ploiement.\",\n",
    "    \"Le bug est li√© √† une mauvaise gestion des erreurs dans le module audio.\",\n",
    "    \"On utilise une architecture microservices avec des conteneurs Docker.\",\n",
    "    \"Les tests unitaires doivent couvrir au moins quatre-vingts pourcent du code.\",\n",
    "    \"J'ai trouv√© la source du probl√®me dans la fonction de traitement audio.\",\n",
    "    \"La latence du pipeline doit rester inf√©rieure √† deux secondes.\",\n",
    "    \"Le mod√®le de langage tourne en local sur le processeur neural Apple.\",\n",
    "    \"Il faudrait optimiser l'algorithme de d√©tection de fin de parole.\",\n",
    "    \"Les logs indiquent une fuite m√©moire dans le gestionnaire de ressources.\",\n",
    "    \"On peut am√©liorer les performances en utilisant le cache de second niveau.\",\n",
    "    # Conversation\n",
    "    \"Franchement je pense que cette solution est la meilleure.\",\n",
    "    \"Tu vois ce que je veux dire, c'est vraiment important pour nous.\",\n",
    "    \"En fait la situation est plus compliqu√©e que ce qu'on croyait.\",\n",
    "    \"Je ne sais pas trop comment aborder ce sujet avec lui.\",\n",
    "    \"C'est exactement √ßa, tu as tout √† fait compris.\",\n",
    "    \"Attends, laisse-moi r√©fl√©chir deux secondes avant de r√©pondre.\",\n",
    "    \"Honn√™tement, je ne suis pas s√ªr que ce soit la meilleure approche.\",\n",
    "    \"On pourrait essayer d'une autre fa√ßon, √ßa me semble plus simple.\",\n",
    "    \"J'ai l'impression qu'on tourne en rond sur ce sujet.\",\n",
    "    \"Ce que tu proposes est int√©ressant mais √ßa demande plus de temps.\",\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ Simulation des erreurs typiques de Whisper FR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "FILLERS = ['euh ', 'heu ', 'bah ', 'ben ', 'du coup ', 'genre ', 'voil√† ', \n",
    "           'en fait ', 'quoi ', 'hein ', 'bon ben ', 'disons ']\n",
    "\n",
    "def inject_whisper_errors(clean: str) -> str:\n",
    "    \"\"\"Simule les erreurs typiques produites par Whisper en FR.\"\"\"\n",
    "    raw = clean\n",
    "    \n",
    "    # Retirer la ponctuation finale (Whisper l'omet parfois)\n",
    "    if random.random() < 0.4:\n",
    "        raw = re.sub(r'[.!?]$', '', raw)\n",
    "    \n",
    "    # Minuscule initiale (Whisper parfois)\n",
    "    if random.random() < 0.3 and raw:\n",
    "        raw = raw[0].lower() + raw[1:]\n",
    "    \n",
    "    # Injecter des fillers au d√©but\n",
    "    if random.random() < 0.5:\n",
    "        raw = random.choice(FILLERS) + raw\n",
    "    \n",
    "    # Injecter des fillers au milieu\n",
    "    if random.random() < 0.3:\n",
    "        words = raw.split()\n",
    "        if len(words) > 3:\n",
    "            pos = random.randint(1, len(words) - 1)\n",
    "            filler = random.choice(FILLERS).strip()\n",
    "            words.insert(pos, filler)\n",
    "            raw = ' '.join(words)\n",
    "    \n",
    "    # B√©gaiement (mot r√©p√©t√©)\n",
    "    if random.random() < 0.25:\n",
    "        words = raw.split()\n",
    "        if len(words) > 2:\n",
    "            pos = random.randint(0, len(words) - 1)\n",
    "            words.insert(pos + 1, words[pos])\n",
    "            raw = ' '.join(words)\n",
    "    \n",
    "    # Espace apr√®s apostrophe (bug Whisper FR)\n",
    "    if random.random() < 0.4:\n",
    "        raw = re.sub(r\"([jcnldJ])'([a-zA-Z√Ä-√ø])\", r\"\\1' \\2\", raw)\n",
    "    \n",
    "    # Double ponctuation\n",
    "    if random.random() < 0.15:\n",
    "        raw = re.sub(r'([.?!])', r'\\1\\1', raw, count=1)\n",
    "    \n",
    "    return raw\n",
    "\n",
    "\n",
    "def format_prompt(raw: str, mode: str = 'chat') -> str:\n",
    "    \"\"\"Format prompt instruction pour le fine-tuning.\"\"\"\n",
    "    mode_instructions = {\n",
    "        'chat': 'Corrige ce texte transcrit par Whisper : supprime les fillers (euh, du coup, genre...), corrige les b√©gaiements, normalise la ponctuation et la majuscule initiale. Conserve le style et le ton naturel.',\n",
    "        'pro':  'Reformule ce texte transcrit par Whisper en style professionnel : supprime les fillers, corrige les b√©gaiements, am√©liore la formulation pour un email ou document formel.',\n",
    "        'code': 'Corrige ce texte transcrit par Whisper en pr√©servant le vocabulaire technique. Supprime uniquement les fillers et b√©gaiements √©vidents.',\n",
    "    }\n",
    "    instruction = mode_instructions[mode]\n",
    "    return f'<|im_start|>system\\nTu es un assistant de correction de dict√©e vocale fran√ßaise.<|im_end|>\\n<|im_start|>user\\n{instruction}\\n\\nTexte brut : {raw}<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ G√©n√©ration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "random.seed(42)\n",
    "N_SAMPLES = 10_000\n",
    "dataset = []\n",
    "\n",
    "modes = ['chat', 'chat', 'chat', 'pro', 'code']  # 60% chat, 20% pro, 20% code\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    clean = random.choice(CLEAN_SENTENCES)\n",
    "    # Variations : combiner 2 phrases parfois\n",
    "    if random.random() < 0.2:\n",
    "        clean2 = random.choice(CLEAN_SENTENCES)\n",
    "        if clean2 != clean:\n",
    "            clean = clean + ' ' + clean2\n",
    "    \n",
    "    raw = inject_whisper_errors(clean)\n",
    "    mode = random.choice(modes)\n",
    "    \n",
    "    prompt = format_prompt(raw, mode)\n",
    "    full_text = prompt + clean + '<|im_end|>'\n",
    "    \n",
    "    dataset.append({\n",
    "        'text': full_text,\n",
    "        'raw': raw,\n",
    "        'clean': clean,\n",
    "        'mode': mode,\n",
    "    })\n",
    "\n",
    "# M√©langer et diviser train/val\n",
    "random.shuffle(dataset)\n",
    "split = int(len(dataset) * 0.95)\n",
    "train_data = dataset[:split]\n",
    "val_data   = dataset[split:]\n",
    "\n",
    "print(f'‚úÖ Dataset g√©n√©r√© : {len(train_data)} train + {len(val_data)} val')\n",
    "print(f'\\nExemple :')\n",
    "ex = train_data[0]\n",
    "print(f'  Brut  : {ex[\"raw\"]}')\n",
    "print(f'  Propre: {ex[\"clean\"]}')\n",
    "print(f'  Mode  : {ex[\"mode\"]}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Optionnel) Enrichir avec vos propres logs\n",
    "\n",
    "Si vous avez des fichiers de logs r√©els de l'application, uploadez-les ici.\n",
    "Format attendu : CSV avec colonnes `raw` et `clean`, ou JSON `[{\"raw\": ..., \"clean\": ...}]`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import files\n",
    "import json, csv\n",
    "\n",
    "USE_REAL_LOGS = False  # Mettre True si vous avez des logs r√©els √† uploader\n",
    "\n",
    "if USE_REAL_LOGS:\n",
    "    print('üìÅ S√©lectionnez votre fichier de logs (.json ou .csv)')\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename, content in uploaded.items():\n",
    "        if filename.endswith('.json'):\n",
    "            real_data = json.loads(content)\n",
    "        elif filename.endswith('.csv'):\n",
    "            import io\n",
    "            reader = csv.DictReader(io.StringIO(content.decode('utf-8')))\n",
    "            real_data = list(reader)\n",
    "        else:\n",
    "            print(f'Format non support√© : {filename}')\n",
    "            continue\n",
    "        \n",
    "        real_formatted = []\n",
    "        for item in real_data:\n",
    "            if 'raw' in item and 'clean' in item:\n",
    "                mode = item.get('mode', 'chat')\n",
    "                prompt = format_prompt(item['raw'], mode)\n",
    "                real_formatted.append({'text': prompt + item['clean'] + '<|im_end|>', **item})\n",
    "        \n",
    "        # Les donn√©es r√©elles ont 3√ó plus de poids (r√©p√©t√©es 3x)\n",
    "        train_data.extend(real_formatted * 3)\n",
    "        print(f'‚úÖ {len(real_formatted)} exemples r√©els ajout√©s (√ó3 = {len(real_formatted)*3} paires)')\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    print(f'Dataset total apr√®s enrichissement : {len(train_data)} exemples')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è  Entra√Ænement sur dataset synth√©tique uniquement.')\n",
    "    print('   Mettez USE_REAL_LOGS = True pour utiliser vos propres logs.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement du mod√®le de base (Task 21 ‚Äî d√©but)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Quantification 4-bit pour √©conomiser la VRAM (T4 = 16 Go)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f'‚è≥ Chargement du tokenizer {MODEL_ID}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(f'‚è≥ Chargement du mod√®le {MODEL_ID} en QLoRA 4-bit...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "vram_used = torch.cuda.memory_allocated() / 1e9\n",
    "print(f'‚úÖ Mod√®le charg√© ‚Äî VRAM utilis√©e : {vram_used:.1f} Go')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration LoRA et entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# Pr√©parer le mod√®le pour le fine-tuning QLoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'‚úÖ LoRA configur√© ‚Äî param√®tres entra√Ænables : {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)')\n",
    "\n",
    "# Convertir en datasets HuggingFace\n",
    "hf_train = Dataset.from_list([{'text': d['text']} for d in train_data])\n",
    "hf_val   = Dataset.from_list([{'text': d['text']} for d in val_data])\n",
    "\n",
    "# Configuration entra√Ænement\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type='cosine',\n",
    "    report_to='none',\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    dataset_text_field='text',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print('‚è≥ D√©marrage de l\\'entra√Ænement...')\n",
    "print(f'   Epochs : {EPOCHS} | Batch effectif : {BATCH_SIZE * GRAD_ACCUM} | LR : {LEARNING_RATE}')\n",
    "trainer.train()\n",
    "print('‚úÖ Entra√Ænement termin√© !')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation rapide du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model.eval()\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, \n",
    "                device_map='auto', max_new_tokens=128, temperature=0.0, do_sample=False)\n",
    "\n",
    "test_cases = [\n",
    "    ('euh du coup je voulais vous dire que genre c est vraiment important hein',   'chat'),\n",
    "    ('j ai besoin d aide pour euh configurer mon ordinateur',                      'chat'),\n",
    "    ('suite √† notre √©change je je vous adresse ce compte-rendu',                   'pro'),\n",
    "    ('le le bug est li√© √† une mauvaise gestion des erreurs dans le module audio',  'code'),\n",
    "]\n",
    "\n",
    "print('‚îÄ‚îÄ Validation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')\n",
    "for raw, mode in test_cases:\n",
    "    prompt = format_prompt(raw, mode)\n",
    "    output = pipe(prompt)[0]['generated_text']\n",
    "    # Extraire uniquement la r√©ponse de l'assistant\n",
    "    response = output.split('<|im_start|>assistant\\n')[-1].split('<|im_end|>')[0].strip()\n",
    "    print(f'[{mode:4s}] Brut  : {raw}')\n",
    "    print(f'       Corrig√© : {response}')\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export GGUF Q4 (Task 22)\n",
    "\n",
    "Fusionne les poids LoRA, exporte en GGUF Q4_K_M pour llama.cpp."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from peft import PeftModel\n",
    "import shutil\n",
    "\n",
    "MERGED_DIR = '/content/qwen25-dictation-fr-merged'\n",
    "\n",
    "# 1. Fusionner LoRA dans le mod√®le de base\n",
    "print('‚è≥ Fusion des poids LoRA...')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16, device_map='cpu', trust_remote_code=True\n",
    ")\n",
    "merged = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "merged = merged.merge_and_unload()\n",
    "merged.save_pretrained(MERGED_DIR)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "print(f'‚úÖ Mod√®le fusionn√© sauvegard√© dans {MERGED_DIR}')\n",
    "\n",
    "# 2. Cloner llama.cpp pour la conversion\n",
    "print('‚è≥ Installation llama.cpp pour conversion GGUF...')\n",
    "!git clone -q --depth 1 https://github.com/ggerganov/llama.cpp /content/llama.cpp\n",
    "!pip install -q -r /content/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt\n",
    "\n",
    "# 3. Conversion HuggingFace ‚Üí GGUF F16\n",
    "print('‚è≥ Conversion vers GGUF F16...')\n",
    "!python /content/llama.cpp/convert_hf_to_gguf.py \\\n",
    "    {MERGED_DIR} \\\n",
    "    --outfile /content/qwen25-dictation-fr-f16.gguf \\\n",
    "    --outtype f16\n",
    "\n",
    "# 4. Quantification Q4_K_M\n",
    "print('‚è≥ Quantification Q4_K_M...')\n",
    "!cd /content/llama.cpp && cmake -B build -DCMAKE_BUILD_TYPE=Release > /dev/null 2>&1\n",
    "!cd /content/llama.cpp && cmake --build build --config Release -j $(nproc) > /dev/null 2>&1\n",
    "!/content/llama.cpp/build/bin/llama-quantize \\\n",
    "    /content/qwen25-dictation-fr-f16.gguf \\\n",
    "    /content/{GGUF_NAME} \\\n",
    "    Q4_K_M\n",
    "\n",
    "size_mb = os.path.getsize(f'/content/{GGUF_NAME}') / 1e6\n",
    "print(f'\\n‚úÖ Export termin√© : {GGUF_NAME} ({size_mb:.0f} Mo)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. T√©l√©chargement du mod√®le fine-tun√©"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "gguf_path = f'/content/{GGUF_NAME}'\n",
    "\n",
    "if os.path.exists(gguf_path):\n",
    "    size_mb = os.path.getsize(gguf_path) / 1e6\n",
    "    print(f'üì• T√©l√©chargement de {GGUF_NAME} ({size_mb:.0f} Mo)...')\n",
    "    files.download(gguf_path)\n",
    "    print()\n",
    "    print('‚úÖ Fichier t√©l√©charg√© !')\n",
    "    print()\n",
    "    print('üìã Prochaine √©tape ‚Äî int√©gration dans l\\'app :')\n",
    "    print(f'   mkdir -p ~/Library/Application\\ Support/dictation-ia/models/')\n",
    "    print(f'   mv ~/Downloads/{GGUF_NAME} ~/Library/Application\\ Support/dictation-ia/models/')\n",
    "    print()\n",
    "    print('   Dans l\\'app, v√©rifier que le mod√®le est d√©tect√© via Settings ‚Üí LLM.')\n",
    "else:\n",
    "    print(f'‚ùå Fichier {gguf_path} introuvable. V√©rifiez les erreurs dans la cellule pr√©c√©dente.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## R√©sum√©\n",
    "\n",
    "| √âtape | Description | Dur√©e estim√©e |\n",
    "|-------|-------------|---------------|\n",
    "| Dataset | 10 000 paires synth√©tiques FR | ~30s |\n",
    "| Chargement mod√®le | Qwen2.5-0.5B-Instruct en QLoRA 4-bit | ~3min |\n",
    "| Entra√Ænement | 3 epochs, batch effectif 32 | ~2-3h (T4) / ~45min (A100) |\n",
    "| Export GGUF | Fusion LoRA + quantification Q4_K_M | ~15min |\n",
    "| **Total** | | **~3-4h (T4)** |\n",
    "\n",
    "**Taille du mod√®le final** : ~400 Mo (Q4_K_M)\n",
    "\n",
    "**RAM requise dans l'app** : ~320 Mo (charg√© √† la demande)"
   ]
  }
 ]
}
